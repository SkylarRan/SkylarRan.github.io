[{"title":"Vector源码分析","url":"/2020/04/28/Vector源码分析/","content":"\n# Vector\n\n## 1. 同步\n\nVector和ArrayList类似，实现了可扩展的对象数组。支持快速随机访问。不同在于Vector是线程安全的，使用synchronized进行同步。\n\n<!-- more -->\n\n```java\npublic class Vector<E>\nextends AbstractList<E>\nimplements List<E>, RandomAccess, Cloneable, Serializable\n\n// synchronized修饰方法，作用的是对象实例\npublic synchronized boolean add(E e) {\n\tmodCount++;\n    ensureCapacityHelper(elementCount + 1);\n    elementData[elementCount++] = e;\n    return true;\n}\n```\n\n## 2. 扩容\n\nVector的构造函数传入capacityIncrement参数，它的作用是在扩容时使容量capacity增长capacityIncrement。若capacityIncrement 小于等于0，则扩容为原来的2倍。\n\n```java\n// Vector容量自动增加的量\nprotected int capacityIncrement;\n\npublic Vector(int initialCapacity, int capacityIncrement) {\n    super();\n    if (initialCapacity < 0)\n        throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                           initialCapacity);\n    this.elementData = new Object[initialCapacity];\n    this.capacityIncrement = capacityIncrement;\n}\n\nprivate void grow(int minCapacity) {\n    // overflow-conscious code\n    int oldCapacity = elementData.length;\n    int newCapacity = oldCapacity + ((capacityIncrement > 0) ?\n                                     capacityIncrement : oldCapacity);\n    if (newCapacity - minCapacity < 0)\n        newCapacity = minCapacity;\n    if (newCapacity - MAX_ARRAY_SIZE > 0)\n        newCapacity = hugeCapacity(minCapacity);\n    elementData = Arrays.copyOf(elementData, newCapacity);\n}\n```\n\n调用没有capacityIncrement参数的构造函数，默认capacityIncrement为0，即默认情况下Vector每次扩容都是原来的两倍。\n\n```Java\npublic Vector(int initialCapacity) {\n    this(initialCapacity, 0);\n}\n\npublic Vector() {\n    this(10);\n}\n```\n\n\n\n## 3. 与ArrayList的比较\n\n- Vector是同步的，所以开销比ArrayList更大，访问速度更慢。 最好用ArrayList而不是Vector，因为同步操作完全可以由程序员控制。\n- 默认情况下，Vector扩容为原来的2倍，而ArrayList扩容是原来的1.5倍。\n\n## 4. 替代方案\n\n- 可以使用`Collections.synchronizedList()` 得到一个线程安全的ArrayList。\n\n```java\nList<Integer> list = new ArrayList<>();\nList<Integer> synList = Collections.synchronizedList(list);\n```\n\n- 使用concurrent并发包下的CopyOnWriteArrayList类\n\n```java\nList<String> list = new CopyOnWriteArrayList<>();\n```\n\n### 补充：CopyOnWriteArrayList\n\n1. 读写分离\n\n   读操作在原始数组中进行，写操作在另一个复制的数组上进行，读写分离，互不影响。\n\n   写操作需要加锁，防止并发写入时写入数据丢失。\n\n   写操作结束后要把原始数组指向新的复制数组。\n\n   ```java\n   public boolean add(E e) {\n       final ReentrantLock lock = this.lock;\n       lock.lock();\n       try {\n           Object[] elements = getArray();\n           int len = elements.length;\n           Object[] newElements = Arrays.copyOf(elements, len + 1);    \n           newElements[len] = e;\n           setArray(newElements);\n           return true;\n       } finally {\n           lock.unlock();\n       }\n   }\n   \n   // array指向新的数组\n   final void setArray(Object[] a) {\n       array = a;\n   }\n   \n   // 告诉编译器忽略 unchecked 警告信息\n   @SuppressWarnings(\"unchecked\")\n   private E get(Object[] a, int index) {\n       return (E) a[index];\n   }\n   ```\n\n   \n\n2. 适用场景\n\n   CopyOnWriteArrayList在写操作的同时允许读操作，大大提高了读操作的性能，适用于读多写少的应用场景。\n\n   缺陷：\n\n   - 内存占用：下操作时同时存在原始数组和复制数组，使得内存占用是原来的两倍。\n   - 数据不一致：读操作不能实时读取数据，因为写操作的新数据还未同步到读数组中。\n\n   所以CopyOnWriteArrayList不适用于对内存敏感且对实时性要求很高的场景。\n\n","tags":["Java容器"],"categories":["Java","Java容器"]},{"title":"ArrayList源码分析","url":"/2020/04/27/ArrayList源码分析/","content":"# ArrayList\n\n## 1. 概览\n\nArrayList基于数组实现，支持快速随机访问。RandomAccess接口标示着该类支持随机访问。\n<!-- more -->\n\n```java\npublic class ArrayList<E>\nextends AbstractList<E>\nimplements List<E>, RandomAccess, Cloneable, Serializable\n```\n\n容量大小默认为10。\n\n```java\nprivate static final int DEFAULT_CAPACITY = 10;\n```\n\n## 2. 扩容\n\n添加元素时使用ensureCapacityInternal（）来保证容量足够，如果不够，需要grow（）来进行扩容，`oldCapacity + (oldCapacity >> 1)`，也就是旧容量的1.5倍。\n\n扩容操作要调用`Arrays.copyOf()`  把原数组元素复制到新数组中，这个操作的代价很高，所以最好在创建ArrayList对象时就指定大概的容量大小，减少扩容操作的次数。\n\n```java\npublic boolean add(E e) {\n    ensureCapacityInternal(size + 1);  // Increments modCount!!\n    elementData[size++] = e;\n    return true;\n}\n\nprivate void ensureCapacityInternal(int minCapacity) {\n    ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));\n}\n\nprivate static int calculateCapacity(Object[] elementData, int minCapacity) {\n    if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {\n        return Math.max(DEFAULT_CAPACITY, minCapacity);\n    }\n    return minCapacity;\n}\n\nprivate void ensureExplicitCapacity(int minCapacity) {\n    modCount++;\n\n    // overflow-conscious code\n    if (minCapacity - elementData.length > 0)\n        grow(minCapacity);\n}\n\nprivate void grow(int minCapacity) {\n    // overflow-conscious code\n    int oldCapacity = elementData.length;\n    int newCapacity = oldCapacity + (oldCapacity >> 1);\n    if (newCapacity - minCapacity < 0)\n        newCapacity = minCapacity;\n    if (newCapacity - MAX_ARRAY_SIZE > 0)\n        newCapacity = hugeCapacity(minCapacity);\n    // minCapacity is usually close to size, so this is a win:\n    elementData = Arrays.copyOf(elementData, newCapacity);\n}\n\n```\n\n## 3. 删除元素\n\n需要调用`System.arraycopy()` 将index + 1后面的元素都复制到index 上，时间复杂度为O（n），所以ArrayList删除元素的代价也是很高的。\n\n```java\npublic E remove(int index) {\n    rangeCheck(index);\n\n    modCount++;\n    E oldValue = elementData(index);\n\n    int numMoved = size - index - 1;\n    if (numMoved > 0)\n        System.arraycopy(elementData, index+1, elementData, index,\n                         numMoved);\n    elementData[--size] = null; // clear to let GC do its work\n\n    return oldValue;\n}\n```\n\n## 4. 序列化\n\n- 序列化：将对象写入到IO流中\n- 反序列化：从IO流中输出对象\n- 意义：序列化机制可以将Java对象转化成位字节序列，这些位字节序列可以保存在磁盘上，或者通过网络传输，恢复成原来的对象。序列化机制可以使对象脱离程序的运行而独立存在。\n- 应用场景：需要进行网络传输的对象或需要保存在磁盘上的对象都必须使可序列化的。 通常建议：程序创建的每个JavaBean类都实现Serializable接口。\n\nArrayList基于数组实现，具有动态扩容的特性，因此保存元素的数组不一定都会被使用，所以就没必要全部进行序列化。\n\n保存数组的elementData由transient修饰，表示数组被默认不会序列化。\n\n```java\ntransient Object[] elementData; // non-private to simplify nested class access\n```\n\nArrayList实现了writeObject()和 readObject()来控制只序列化有元素填充的那部分。\n\n```java\nprivate void writeObject(java.io.ObjectOutputStream s)\n    throws java.io.IOException{\n    // Write out element count, and any hidden stuff\n    int expectedModCount = modCount;\n    s.defaultWriteObject();\n\n    // Write out size as capacity for behavioural compatibility with clone()\n    s.writeInt(size);\n\n    // Write out all elements in the proper order.\n    for (int i=0; i<size; i++) {\n        s.writeObject(elementData[i]);\n    }\n\n    if (modCount != expectedModCount) {\n        throw new ConcurrentModificationException();\n    }\n}\n\nprivate void readObject(java.io.ObjectInputStream s)\n    throws java.io.IOException, ClassNotFoundException {\n    elementData = EMPTY_ELEMENTDATA;\n\n    // Read in size, and any hidden stuff\n    s.defaultReadObject();\n\n    // Read in capacity\n    s.readInt(); // ignored\n\n    if (size > 0) {\n        // be like clone(), allocate array based upon size not capacity\n        int capacity = calculateCapacity(elementData, size);\n        SharedSecrets.getJavaOISAccess().checkArray(s, Object[].class, capacity);\n        ensureCapacityInternal(size);\n\n        Object[] a = elementData;\n        // Read in all elements in the proper order.\n        for (int i=0; i<size; i++) {\n            a[i] = s.readObject();\n        }\n    }\n}\n\n```\n\n序列化时需要使用ObjectOutputStream的writeObject() 将对象转化为字节流并输出。**而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。**\n\n反序列化使用ObjectInputStream的readObject() ,原理类似。\n\n```java\nArrayList list = new Arratlist();\nObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));\noos.writeObject(list); // 这段代码会通过反射让list调用它的list.writeObject()\n```\n\n## 5. Fail-Fast\n\n- “fail-fast”\n\n  - 也就是“快速失败”，它是Java集合的一种错误检测机制。某个线程在对集合进行迭代时，不允许其他线程对该集合进行结构上的修改。若迭代时出现结构性改变，会抛出ConcurrentModificationException异常。\n  - 迭代器的快速失败行为无法得到保证，它不能保证一定会出现该错误，因此，ConcurrentModificationException应该仅用于检测 bug。\n  - Java.util包中的所有集合类都是快速失败的，而java.util.concurrent包中的集合类都是安全失败的；\n    快速失败的迭代器抛出ConcurrentModificationException，而安全失败的迭代器从不抛出这个异常。\n\n- modCount\n\n  - ArrayList继承了抽象类AbstractList，同时也就拥有了AbstractList中的全局变量modCount，用来记录ArrayList发生结构性改变的次数。结构性改变指的是添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小操作，在迭代过程中可能会造成错误的结果。仅仅只是设置元素的值不算结构发生变化。\n\n  - modCount交由迭代器（Iterator）和列表迭代器（ListIterator）使用，当进行next()、remove()、previous()、set()、add()等操作时，如果modCount的值意外改变，那么迭代器或者列表迭代器就会抛出ConcurrentModificationException异常。代码参考上一点序列化函数实现。\n\n## 总结\n\n- ArrayList基于数组实现，支持快速随机访问。\n- 数组容量大小默认为10，动态扩容的大小是原来的1.5倍。\n- 插入和删除操作涉及元素的移动，操作代价是极高的，故最好在创建时就指定大概的数组大小。\n- 数组默认不用序列化，ArrayList中实现了读写对象的方法，用来控制对已填充元素的那部分进行序列化。","tags":["Java容器"],"categories":["Java","Java容器"]},{"title":"Java容器","url":"/2020/04/27/概览/","content":"\n# 一、 概览\n\n容器分为Collection和Map两种， Collection是存储（单个）对象的集合，Map是存储键值对（两个对象）的映射表。\n<!-- more -->\n\n## Collection\n\n![collection](/img/java/container/collection.png)\n\n1. Set\n\n   - set 存放的元素不能重复\n\n   - TreeSet：基于红黑树实现，O（logN）\n   - HashSet：基于哈希表实现，支持快速查找，O（1），元素无序。 失去了元素的顺序信息，意味着使用 Iterator 遍历的结果是不确定的。\n   - LinkedHashSet：具有HashSet的查找效率，内部使用的是双向链表维护元素的插入顺序。\n\n2. List\n\n   - list 可以存放重复元素\n   - ArrayList：基于动态数组实现，支持随机访问\n   - Vector：和ArrayList类似，但它是线程安全的\n   - LinkedList：基于双向链表实现，只能按顺序访问，但插入和删除元素迅速。LinkedList还可用作栈、队列和双向队列。\n\n3. Queue\n\n   - queue “先进先出”\n   - LinkedList： 可用作队列和双向队列\n   - PriorityQueue：基于堆结构实现，用作优先队列\n\n\n## Map\n\n![map](/img/java/container/map.png)\n\n- HashMap：基于哈希表实现。\n- HashTable：和HashMap类似，它是线程安全的。但它是遗留类，不应该使用它，而是使用ConcurrentHasgMap来支持线程安全。ConcurrentHashMap的执行效率更高，因为它引用了分段锁。\n- LinkedHashMap：使用双向链表来维护元素的顺序，顺序是插入顺序或最近最少使用（LRU）顺序。\n- TreeMap：基于红黑树实现。\n\n# 二、容器中的设计模式\n\n## 迭代器模式\n\n![img](https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/image-20191208225301973.png)\n\nCollection接口继承了Iterable接口，Iterable接口中的Iterator( )方法产生（返回） 一个Iterator对象，通过这个对象就可以迭代遍历集合容器中的元素。\n\n```java\nList<Integer> list = new ArrayList<>();\nlist.add(1);\nlist.add(2);\n// 1. 使用迭代器迭代\nIterator<Integer> it = list.iterator();\nwhile(it.hasNext()){\n  \tint i = it.next();\n}\n\n// 2.使用forEach迭代\nfor(int i: list){}\n```\n\n## 适配器模型\n\n例如：java.util.Arrays 的 aslist( )方法可以把数组类型转换成List类型。\n\n```java\n@SafeVarargs\npublic static <T> List<T> asList(T... a)\n```\n\n注意asList方法的参数为泛型的变长参数，不能使用基本类型数组作为参数，要使用对应的封装类型数组\n\n```java\nInteger[] arr = {1,2,3};\nList list = Arrays.asList(arr);\n    \n// 也可以直接按以下方式调用\nList list = Arrays.asList({1,2,3});\n```\n\n","tags":["Java容器"],"categories":["Java","Java容器"]},{"title":"15.StringMatch","url":"/2019/12/27/15.StringMatch/","content":"# 字符串匹配\n\n## BF算法（Brute Force）\n- 主串:len = n\n- 模式串:len = m    \n- 在主串中查找模式串： n>m\n- 最简单、最暴力的匹配查找\n\n思想： 分别从下标为0、1、...、n-m开始且长度为m的子串与模式串进行比较，共有n-m+1个子串。 每次都要遍历m个字符进行比较，最差情况下要进行n-m+1次，故最坏时复为O（m*n）\n\n![](../img/stringMatchBF.jpg)\n\n理论上BF算法的时复很高，但实际开发中使用却很常见，大部分情况下使用BF进行字符串匹配就足够了。原因有两点：\n\n1. 实际应用的主串与模式串都不太会长。子串与模式串进行比较时，遇到不相等字符就停止了，不会总是进行m次比较。所以大部分情况下的执行效率比时复O（m*n）高很多。\n2. BF算法思想最简单，意味着代码简单不易出错，有bug了也易修复。在工程中，满足性能的前提下，简单是首选，即KISS设计原则（keep it simple and stupid）\n\n## RK算法（Rabin-Karp）"},{"title":"14Graph","url":"/2019/12/27/14Graph/","content":"# 图\n顶点（vertex） 边（edge） 度（degree）\n有向图： 入度（in-degree） 出度（out-degree）\n带权图（weighted graph）\n\n## 图的存储\n### 邻接矩阵（Adjacency Matrix）\n![](../img/graph_adjacency_matrix.jpg)\n\n使用邻接矩阵存储图，简单直观，但比较浪费内存空间。如：无向图中邻接矩阵是对角线上下对称的，稀疏图中边很少，对应的邻接矩阵很多位置都是0。邻接矩阵的优点是基于数组可以快速查看两顶点中是否有边。\n\n### 邻接表（Adjacency List）\n![](../img/graph_adjacency_list.jpg)\n\n针对邻接矩阵浪费内存的问题，设计了邻接表存储图。结构与散列表+拉链相似。对于无向图来说，每个顶点后面的链表存放的就是与该顶点之间有边的所有顶点。\n\n若要查看某两个顶点A与B之间是否有关系，则要在A顶点的链表中查找是否有B，若是单链表的结构，查找时复为O（n），查找速度太慢，可以将单链表改进为高效的动态数据结构，如跳表、散列表、平衡二叉查找树等，或是改成有序数组使用二分查找。\n\n邻接矩阵费内存，查找速度快，属于空间换时间；邻接表省内存，查找速度较慢，属于时间换空间。\n\n## 如何存储微博中的好友关系？\n1. 判断A是否关注了B\n2. 判断B是否是A的粉丝（A被B关注）\n3. 用户A关注B\n4. 用户A取消关注B\n5. 根据用户名称的首字母排序，分页获取A的关注列表\n6. 根据用户名称的首字母排序，分页获取A的粉丝列表\n\n根据需求可知好友关系是一个有向图，出度即为关注，入度即为粉丝，采用邻接表省空间，要维护关注用户对应的邻接表和粉丝用户对应的逆邻接表。因为用户众多，单靠邻接表很容易查找关注用户，但查找粉丝用户则很困难，故额外再维护一个逆邻接表。\n\n![](../img/weibo_graph.jpg)\n\n基础的邻接表不适合快速判断两个用户之间是否有关注与被关注关系，所以将邻接表中的链表改进为支持快速查找的动态数据结构。要按用户名称首字母排序分页获取关注与粉丝列表，这里采用跳表最合适，因为跳表是已排序的，查找插入与删除都很高效，时复为O（logn），要存储索引，空复为O（n）。\n\n对于小规模数据，如几万、几十万用户数据，可以直接用跳表存放在内存中。但对于微博上亿用户的大规模数据，内存是不够的。\n\n1. 通过哈希算法等数据分片方式，将邻接表存放在多台机器上，例如将顶点1,2,3的邻接表和逆邻接表存放在机器1上，将顶点4,5的邻接表和逆邻接表存放在机器2上。当要查询顶点间的关系时，先利用哈希算法定位到机器上，再查找邻接表。\n\n2. 利用外部存储（硬盘等，存储空间非常大），数据库的持久化存储方式。建表存放user_id与follow_id。\n\n## 广度优先搜索（bfs）与深度优先搜索（dfs）\n\n两者都需要使用visited数组记录已访问的顶点，使用prev数组记录访问路径\n\nbfs借助队列对图进行按层遍历（记录层数可以应用于寻找n度好友）\n\ndfs借助栈使用递归缩短搜索路径（设置递归的深度同样可以寻找n度好友），同时需要一个布尔变量found来结束递归函数。\n\n都需要对所有顶点和边进行遍历，连通图中边数E>=顶点数V-1，bfs和dfs的时复为O（E+V），也可以写成O（E）\n\n借助了额外的内存空间visited、队列或栈，否不超过顶点总数V，故空复为O（V）\n\n## 如何将迷宫抽象成一个图？即计算机如何存储迷宫？\n设置迷宫的入口和出口为开始顶点s和结束顶点t，从入口开始随机选一条路走，每走到分叉口建立一个顶点，且与上一个顶点建立一条边，给边加上权重用来记录两点间的距离，走到死胡同就进行回溯。最后利用bfs或dfs可以找到s到t的唯一一条路径。"},{"title":"11.Tree","url":"/2019/12/27/11.Tree/","content":"# 树\n\n![Alt](../img/tree.jpg)\n\n- 根节点： 没有父节点的节点，E\n- 父节点： A是B的父节点\n- 子节点： B是A的子节点\n- 兄弟节点： B、C、D是兄弟节点\n- 叶子节点： 没有子节点的节点，G、H、I、J、K、L\n\n![Alt](../img/tree_height_deepth_level.jpg)\n\n- 节点的高度：节点到叶子节点的最长路径（边数）\n- 节点的深度：根节点到这个节点的路径（边数）\n- 节点的层数：节点的深度 + 1\n- 树的高度： 根节点的高度\n\n## 二叉树（Binary Tree）\n\n每个节点最多只有左右两个子节点。\n\n- 满二叉树：最后一层全是叶子节点，除去最后一层的叶子节点，其余每个节点都有左右两个子节点\n- 完全二叉树： 最后一层叶子节点的都从左边开始排序，除去最后一层就是满二叉树。\n\n## 二叉树的存储\n1. 基于指针或引用的二叉链式存储法\n    \n    每个结点含有3个属性，data表示存储对象数据，left和right指针分别表示左右子节点。\n\n![Alt](../img/binaryTreeBaseLinkedList.jpg)\n\n2. 基于数组的顺序存储法\n\n    把根节点存放在下标i=1的位置，则它的左子节点的下标为i = 2 * 1 = 2，它的右子节点的下标为i = 2 * 1 + 1 = 3，所有节点X的存放位置下标为i，则它的左子节点存放在2i，右子节点存放在2i+1\n\n![Alt](../img/binaryTreeBaseArray.jpg)\n\n若是一棵完全二叉树，采用数组存储只会浪费下标为0的内存空间。若是一颗普通的树，会浪费数组的多个内存空间。\n\n所以对于完全二叉树，顺序存储是最省内存的方式，因为不需要像链表存储额外的左右子节点的指针，这是完全二叉树要求最后一层子节点靠左的原因。\n\n## 二叉树的遍历\n\n![Alt](../img/binaryTreeTranverse.jpg)\n\n按层遍历：借助队列，按层依次入队， 遍历下一层节点时，先出队拿到节点，再入队该结点存在的左右子节点\n\n## 二叉查找树（Binary Search Tree）\n\n> 任意一个节点，其左子树中每个节点的值都小于该节点的值，其右子树中每个节点的值都大于该节点的值\n\n### 查找\n### 插入\n### 删除\n1. 查找要删除的节点p和它的父节点pp\n2. 要删除的节点p含有两个子节点：\n    - 找到节点p的右子树的最小节点minP和它的父节点minPP\n    - 将节点p的值换成minP的值\n    - 删除节点minP（相当于删除叶子节点或只有一个右子节点的节点）\n\n3. 要删除的节点p含有0或1个子节点：\n    - 获取节点p的孩子节点\n    - 将父节点pp的子节点修改为孩子节点，需判断子节点的左右和p是否是根节点\n\n\n或者直接将要删除的节点标记为“已删除”，这样会浪费内存空间，但删除操作简单，且对查找和插入操作也不影响。\n\n### 支持快速查找最大、最小、前驱、后继节点\n\n### 支出重复数据的查找\n1. 在同一个节点上利用数组或链表存放相同的数据\n2. 把相同的值当作大于节点来看，插入到树结构中。 查找时，遇到值相同的节点记录下来继续查找，直到查到叶子节点为止。删除时，需要先查找所有值相同的节点再进行删除操作。\n![](../img/BSTInsertDuplicateValue.jpg)\n\n## 二叉查找树的时间复杂度分析\n- 时间复杂度跟树的高度成正比， O（height）\n- 每层（高度为h， 层数为i）的最大节点数为 2^h 或 2^(i-1)\n- 完全二叉树最后一层的节点数范围[1, 2^(i-1)]\n- 若有n个节点，则树的高度为\n\n最差情况，二叉树的所有节点都只有左子节点或者只有右子节点，二叉查找树退化成链表，查找、插入与删除时复为O（n）。\n理想情况，二叉查找树是一棵完全二叉树\n\n## 常见题型\n\n1. 验证二叉查找树\n* 中序遍历后，判断数组是否有序，时复O（N），借助额外的数组，空复是O（n）\n    * 若不借助数组，递归中序遍历，直接判断left< mid < right, 还要判断左子树的max < mid < 右子树的min， 第二点怎么判断呢？\n     \n"},{"title":"13.Heap","url":"/2019/12/19/13.Heap/","content":"# 堆\n堆是一种特殊的树，需要满足两点：\n1. 堆是一棵完全二叉树。\n    > 完全二叉树：除去最后一层，其余节点都是满的（满二叉树）， 最后一层的子节点靠左排列。\n2. 堆中每个节点的值都必须大于等于（或小于等于）其子树中每个节点的值\n    > 等价于 堆中每个节点的值都大于等于（或小于等于）其左右子节点的值。 “大于等于”的叫做“大顶堆”，“小于等于”的叫做“小顶堆”。\n\n    ![](../img/heap.jpg)\n\n    上图中1和2是大顶堆， 3是小顶堆，4不是完全二叉树，故不是堆。还可看出，对于同一组数据，可以构建多种形态的堆。\n\n## 堆的实现\n\n堆是一棵完全二叉树，故用数组存储更省空间，其中根节点下标为i=1， 左子节点下标为2 * i， 右子节点下标为2 * i +1， 父节点下标为i/2。\n\n1. 往堆中插入一个元素\n\n    > 若直接将新元素作为叶子节点插入堆中，就破坏了堆的特性，于是需要进行调整，调整的过程叫做堆化（heapify）。堆化有两种方法，从下往上和从上往下。 现在举例从下往上。\n\n    ![](../img/heap_insert.jpg)\n    \n    堆化过程：顺着节点路径向上或者向下，进行对比、交换。\n\n    ![](../img/heapify.jpg)\n\n2. 删除堆顶元素\n\n    > 从堆的第二点定义，堆中的每个节点都大于等于（小于等于）其子树中每个节点的值，可以得知堆顶元素是最大值（最小值）。\n\n    > 以大顶堆为例，堆顶元素为最大值，删除堆顶元素，就需要找到第二大元素放在根节点处。\n\n    ![](../img/heap_delete_1.jpg)\n\n    > 若直接删除堆顶元素，从子节点中找到最大值替换，迭代替换过程，直到叶子节点，但这样会出现数组空洞，就不符合完全二叉树的定义，也就不是堆了。\n\n    > 换个思路，用最后一个节点替换根节点，然后进行从上而下的堆化过程。\n\n    ![](../img/heap_delete_2.jpg)\n\n\n3. 堆化的时复与完全二叉树的高度成正比， 完全二叉树的高度不会超过log2 N，所以堆化的时复为O（logn）。堆的插入与删除操作就是堆化过程，所以堆的插入与删除堆顶元素的时复是O（logn）\n\n## 基于堆实现排序（堆排序：时复O（nlogn），原地排序，不稳定）\n### 建堆（两种方法）\n1. 从下往上堆化， 数组处理从前往后\n    > 所有元素依次插入堆中，成为最后一个叶子节点，然后再每次插入后对最后一个叶子节点进行从下往上堆化。\n\n2. 从上往下堆化， 数组处理从后往前（从第一个非叶子节点开始）\n    > 将所有元素放入数组中， 然后从第一个非叶子节点开始进行从上往下堆化。第一个非叶子节点为中点(已存储的节点个数self.count//2\n\n    ![](../img/heap_create_1.jpg)\n    ![](../img/heap_create_2.jpg)\n\n> 表面上，堆化的时复为O(logn),要堆化的节点个数为n/2,故建堆的时复为O(nlogn)。但这个答案还不够精确。\n\n> 节点在堆化过程中，比较和交换的次数是与该结点的高度k成正比的。如第一个非叶子节点只需要向下比较1次，而根节点要向下比较高度k次。\n\n![](../img/heap_height.jpg)\n\n上图展示了非叶子节点的高度，叶子节点的高度为0。 对所有非叶子节点的高度求和：\n![](../img/heap_height_sum.jpg)\n\n对S1 左右两边*2，得S2，左右两边再相减得S=2^(h+1) - h - 2, 代入h = log2 N 得 S = 2N - log2 N -2 = O(n)。所以，建堆的时间复杂度为O(n)。\n\n\n\n### 排序\n完成建堆后，堆顶元素都是最大元素， 每次都将堆顶元素与数组未排序的最后一个节点交换，也就是依次拿到未排序的最大元素，这样就实现了堆排序。而拿取堆顶元素后相当于删除堆顶元素，进行堆化。\n\n1. 堆排序分为建堆和排序两步，建堆的时复为O(n),排序中遍历n次，每次堆化为logn，时复为O(nlogn)，故堆排序的时复为O(nlogn)。\n2. 堆排序过程中只有交换操作会涉及个别临时空间，故是原地排序。\n3. 堆排序中将最后一个节点元素与堆顶节点元素互换，所以不是稳定的排序算法\n\n## 为什么快排比堆排序性能好?\n快排：一般选择最后一个元素作为分区元素，从第一个元素开始，维护一个指针i从0开始， 若大于分区元素，不动，若小于分区元素，将该元素与下标为i的元素交换，直到分区元素，将分区元素与下标为i的元素交换。\n分区元素选的好，则需log2 N次分区，分区过程中遍历n次，故快排的时复为O(nlogn),不稳定，原地排序。\n\n1. 堆排序数据访问方式没有快排友好\n    > 快排中数据是依次顺序访问的，堆排序中数据是跳着访问的，如堆化过程中数据访问会出现下标为1,2,4,8的情况，而快排是局部顺序访问，这样对CPU缓存不友好。\n\n2. 对于同样的数据，堆排序的交换次数要多于快排\n    > 快排交换的次数不会比逆序度更多，而堆排序的建堆过程可能会增加原始数据的逆序数，从而是交换次数增加。\n\n## 堆的应用"},{"title":"12.RBTree","url":"/2019/12/14/12.RBTree/","content":"# 红黑树\n\n> 二叉查找树在频繁地动态更新过程中，可能会出现树的高度远大于log2 N 的情况，从而导致各个操作的执行效率下降。极端情况下，二叉树会退化成链表，时复退化到O(n)。故要解决复杂度退化的问题，设计出了平衡二叉查找树。\n\n> 平衡二叉查找树中使用最多的就是红黑树。\n\n## 平衡二叉查找树\n平衡二叉树的严格定义：二叉树中任意一个节点的左右子树的高度相差不能超过1.\n\n最先发明的平衡二叉查找树是AVL树，它严格符合平衡二叉树的定义，是一种高度平衡的二叉查找树。\n\n很多平衡二叉查找树并没有严格符合平衡二叉树的定义，比如红黑树，它从根节点到各个叶子节点的最长路径，可能比最短路径大一倍。\n\n## 红黑树的定义\n红黑树的节点被标记为红色或者黑色：\n1. 根节点是黑色\n2. 每个叶子节点都是黑色的空节点（Null），即叶子节点不存储数据（为了简化红黑树的代码实现而设置）\n3. 任何相邻的节点都不能同时为红色，即红色节点是被黑色节点隔开的\n4. 任一节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点\n\n![](../img/RBT.jpg)\n\n## 为什么说红黑树是“近似平衡”的\n平衡二叉查找树是为了使二叉查找树的性能不退化，故“近似平衡”等价于性能不能退化的太严重\n\n一棵极其平衡的二叉查找树的高度大约为log2 N，所以要证明红黑树是近似平衡的，只需要分析红黑树的高度是否稳定地趋近于log2 N。\n\n1. 将红黑树中的红色节点去掉后，部分失去父节点的节点将以原来的祖父节点作为新的父节点，之前的二叉树就变成了四叉树。\n![](../img/RBT1.jpg)\n\n从四叉树中取出部分节点，放到叶子节点的位置，那么就变成了完全二叉树。 所以仅包含黑色节点的四叉树的高度比包含相同节点个数的完全二叉树的高度小。 而完全二叉树的高度近似log2 N，所以去掉红色节点的“黑树”的高度也不会超过log2 N。\n\n2. 在红黑树中，红色节点不能相邻，也就是有一个红色节点就至少有一个黑色节点将其与其他红色节点隔开。红黑树中包含最多黑色节点的路径不会超过log2 N，所以加入红色节点后，最长路径不超过2log2 N。 故红黑树的高度近似2log2 N。\n\n红黑树比高度平衡的AVL树的高度大了一倍，但性能上红黑树更好。AVL树要维护高度平衡要付出很大代价，而红黑树只是近似平衡，所以维护代价相对较低。\n\n## 动态数据结构对比\n\n散列表：借助数组实现查找操作时复为O（1），会发生散列冲突，用开放寻址或者链表法来解决。\n\n跳表：在链表上加入多级索引，查找时复为O（logn）\n\n红黑树：一种平衡二叉查找树，时复为O（logn）\n\n\n\n\n"},{"title":"8.BinarySearch","url":"/2019/12/09/8.BinarySearch/","content":"# 二分查找：如何用最省内存的方式实现快速查找功能？\n二分查找针对的是一个有序集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为原来的一般，知道找到该元素或者区间缩小为0。时间复杂度为O（logn），极其高效。\n\n## 二分查找的递归与非递归实现\n\n## 二分查找应用场景的局限性\n1. 二分查找依赖的是顺序表结构，简单点说就是数组，利用了数组根据下表随机访问元素时复为O（1）的特征。\n2. 二分查找针对的是有序数据\n> 如果数据无序，需要先排好序。 如果针对的是一组静态数据，插入和删除操作较少，可以进行一次排序，多次二分查找。这样排序的成本可被均摊。 \n> 若是有频繁地插入和删除操作,要保证在二分查找前是有序的， 要么在插入和删除数据之后就保证数据仍然有序，要么就在二分查找之前排序。这两种方式维护有序的成本都是很高的。\n> 所以，二分查找只适用于插入与删除操作不频繁，一次排序多次查找的场景。针对动态变化的数据集，二叉树结构更适合查找。\n3. 数据量太小不适合二分查找\n> 二分查找的时复是O（logn），数据规模大才能体现出O（logn）的高效，二分查找的优势才更明显。 若数据量很小，直接顺序遍历查找就好了，小数据规模下两者的执行效率差不多。\n\n> 若是查找过程中的比较操作很复杂，例如长字符串的对比，这时就需要尽量减少比较操作，即使数据量很小，也要选择二分查找来减少比较次数。\n4. 数据量太大也不适合二分查找\n> 二分查找依赖的是数组，数组就要保证数据的连续性，对内存要求比较苛刻。数据量太大很难申请到连续的内存空间来存放数据。\n\n## 假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 我们希望这个功能不要占用太多的内存空间，最多不要超过 100MB，你会怎么做呢？\n1MB = 1000 Kb = 1000000 b\n1000w * 8b  = 80 000 000 b = 80MB\n所以直接将所有数据存放在内存中，先用快排，再进行二分查找。 这种方式是最节省内存空间的。\n散列表和二叉树是支持快速查找的动态数据结构，但在这里不行，因为存放数据会需要额外的空间，100MB肯定不够。\n\n## 如果数据使用链表存储，二分查找的时间复杂就会变得很高，那查找的时间复杂度究竟是多少呢？\n如果数据使用单链表存储且是有序的，使用快慢指针同时获取中间结点尾结点的时复是O（n），再比较元素缩小区间，那么基于有序单链表的二分查找的时复就是O（nlogn）"},{"title":"9.SkipList","url":"/2019/12/09/9.SkipList/","content":"# 跳表\n> 对链表加以改造，便可支持类似“二分”的查找算法，改造后的数据结构就叫做“跳表”\n> 跳表支持快速插入、删除、查找操作，是一种性能较高的动态数据结构\n\n## 理解\n即使对于有序链表，查找某个数，也需要遍历，时复为O（n）。如何提高查找效率呢？建立索引。。\n\n> 建立一级索引，每两个结点提取一个，图中down表示down指针。\n![Alt](../img/skipList1.jpg)\n\n> 在一级索引的基础上，加二级索引\n![Alt](../img/skipList2.jpg)\n\n## 用跳表查询到底有多快？\n设链表有n个结点，一级索引有n/2个结点，第k级索引的结点个数为n/(2^k)\n\n设每两个结点提取，链表共有h级索引，第h级索引有2个结点（3个结点的情况可以当做2个），则 n/(2^h) = 2，h = log2 N - 1 ，加上原始链表，整个跳表的高度是h+1 = log2 N， 若每次遍历m个结点，则时间复杂度是O（m*logn）。\n\n当查找元素为x， 查找第k级索引，发现 y < x < z ,则查找第k-1级索引，要查找y，&，z共三个结点，判断x处于（y，&）还是（&，z）之间，所以每层遍历最多3个结点。故跳表查找操作的时复为O（logn）。\n\n查询效率提升的前提是建立索引，用了空间换时间的思想。\n\n## 跳表要消耗额外的内存空间\n跳表建立索引的结点总数为 n/2 + n/4 + ... 8 + 4 + 2 = n - 2， 故跳表的空复为O（n)。\n也就是说n个结点的单链表构造成的跳表，需要接近n个结点的额外的内存空间。\n\n* 如何才能减少消耗的内存空间呢？\n索引的建立换成每3个结点提取一次，总的索引结点数为n/3 + n/9 + ... + 9 + 3 = n/2， 空复还是O（n），但索引结点数减少了大约一半。\n\n实际开发中，链表存放的不是简单的数字，有可能是很大的对象，这时索引结点占用的空间相对于对象结点来说是可以忽略不计的。\n\n## 高效的动态插入与删除\n\n在单链表中，要插入与删除结点，需要遍历链表，插入与删除操作的时复为O（1），所以总的时复为O（n）\n\n而跳表中查找某个结点的时复O（logn），所以跳表的动态插入与删除操作时复均为O（logn）\n\n![Alt](../img/skipList3.jpg)\n\n## 跳表索引动态更新\n\n若不断地在某两个结点之间插入结点，会造成对应索引结点之间的结点越来越多，使得这一段表现为单链表。所以极端情况下，跳表就退化为单链表了。\n\n为了维护索引与原始链表之间的大小平衡，在往跳表中插入数据时，可以同时往部分索引层中插入该数据。通过随机函数，来决定新的数据插入到哪几级索引中\n![Alt](../img/skipList4.jpg)\n\n关于随机函数的选择可以看Redis 中关于有序集合的跳表实现。\n\n\n"},{"title":"7.4Sort(4)","url":"/2019/12/09/7.4Sort(4)/","content":"# 排序优化：如何实现一个通用的、高性能的排序算法\n\n## 如何选择合适的排序算法？\n![Alt](../img/sortCompare.jpg)\n\n线性排序算法的适用场景比较特殊，所以在写通用的排序算法时不能用线性排序。 \n\n如果对小规模数据排序时，可以选择时复为O（n^2）的算法；对大规模数据排序，要用时复为O（nlogn）的算法。 故通用排序算法首选时复为O（nlogn）的算法。\n\n归并算法最好、最坏、平均情况的时复都是O（nlogn），但是在合并函数有使用额外的临时数组，故空复为O（n）。对于大规模数据来说，使用归并算法空间耗费较大，这是致命缺点。\n\n快速排序是原地且不稳定排序，通常时复为O（nlogn），在最坏情况下，需要进行n次分区，时复为O（n^2）。但对分区点的获取进行优化，可以避免最坏情况。所以快排比归并更受欢迎。\n\n## 如何优化快速排序？\n理想情况：被分区点分开的两个区中，数据的数量差不多。\n如果很粗暴地选择第一个元素或最后一个元素，很容易发生极端情况，使时复退化到O（n^2）。所以我们要选择合适的分区点，尽可能让每次分区都比较均匀。\n\n1. 三数取中法\n从待分区数组中取出首、尾和中间三个元素进行对比，找出中间值作为分区点。\n如果分区数组中元素很多，可以采取“五数取中”或“十数取中”。\n\n2. 随机法\n从待分区数组中随机选取一个元素作为分区点。\n\n注意： 快速排序是递归实现的，而递归要警惕堆栈溢出。为了避免快排因递归过深而堆栈过小，导致堆栈溢出。两种解决办法：\n1. 限制递归深度。 一旦递归的深度超过了事先设定的阈值，就停止递归。\n2. 在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈过程，这样就没了系统栈大小的限制。\n\n## 举例分析排序函数"},{"title":"7.1Sort(1)","url":"/2019/12/09/7.1Sort(1)/","content":"# 冒泡、插入、选择排序 O（n^2）\n\n## 如何分析一个排序算法？\n1. 执行效率\n* 最好、最坏、平均时间复杂度\n    > 明确最好、最坏情况对应的原始数据是什么\n    >\n    > 通过时间复杂度来对比区分排序算法在不同数据下的性能表现。 \n* 时间复杂度的系数、常数和低阶\n    > 往往要排序的数据规模n是确定的，对于同一阶时间复杂度的排序算法性能对比时要考虑时复的系数、常数和低阶\n* 比较次数和交换（移动）次数\n    > 在分析算法的执行效率时，也需要把比较和交换（移动）次数考虑进来。\n2. 内存消耗\n    > 内存消耗是通过空间复杂度来衡量的，在排序算法中，进入一个“原地排序”（sorted in place）的概念。原地排序算法，特指空间复杂度为O（1）的排序算法\n3. 稳定性\n    > 排序数据中若存在值相同的元素， 排序后相同元素未交换位置的算法即为稳定算法。 \n    > \n    > 例如对于订单按照时间从早到晚，金额从小到大排序。 若先按金额排序，在按时间排序，金额相同时间不同的订单可能会交换位置。 若先按时间排序，再按金额排序，相同金额的订单仍然按照时间进行排序，这样排序稳定性更高。 \n\n## 冒泡排序\n1. 进行n-1轮排序，相邻元素比较，每轮排序选出较大值\n2. 优化：某轮排序无数据交换则停止外层循环\n* 空间复杂度为O（1），属于原地排序\n* 相同元素不交换顺序，属于稳定的排序\n* 平均时复O（n^2）\n    > 最好情况:正序, 时复为O（n）,有序度为(n-1)+...+2+1 = n(n-1)/2(满有序度)\n    >\n    > 最坏情况：逆序， 时复为O（n^2），有序度为0\n    >\n    > 平均有序度为n(n-1)/4,即为交换次数，比较次数是多于交换次数的，时复上限为O（n^2），故平均时复为O（n^2）\n\n## 插入排序\n1. 进行n-1轮排序，从后往前比较，将待排元素插入到已排序数组中的正确位置上\n2. 数据移动的个数总和 = 逆序度\n* 属于原地排序\n* 稳定的排序\n* 平均时复O（n^2）\n    >  最好情况:正序, 比较n-1次，时复为O（n）\n    >\n    > 最坏情况：逆序，逆序度为n(n-1)/2，比较与交换次数为逆序度，相当于在数组首位插入元素，时复为O（n^2）\n    >\n    > 数组中插入一个数据的平均时复为O（n），要遍历插入n次，故平均时复为O（n^2）\n\n## 选择排序\n1. 进行n-1轮排序，每次选择待排元素中的最小元素， 交换最小元素与即将待排位置上的元素\n2. 比较次数不变，为n(n-1)/2\n* 属于原地排序\n* 不稳定， 如 3  2 4 3 5 1， 第一轮就把第一个3和1交换了位置，即两个3交换了位置\n* 最好、最坏、平均时复都是O(n^2)，因为比较次数不变，只是交换次数不同。\n\n## \n\n排序算法| 原地排序 | 是否稳定 | 平均时复 |最好时复 | 最坏时复\n--|--|--|--|--|--\n冒泡 | 是 | 是 | O(n^2) | O(n) | O(n^2) \n插入 | 是 | 是 | O(n^2) | O(n) | O(n^2) \n选择 | 是 | 否 | O(n^2) | O(n^2) | O(n^2) \n\n\n## 为什么插入排序比冒泡排序更受欢迎？\n冒泡排序和插入排序的交换（移动）次数都等于原始数据的逆序度，但冒泡排序的交换操作是3句， 插入排序的移动操作只是1句。 实际中插入排序比冒泡排序的执行效率高。要把性能优化做到极致，当然首选插入排序。\n\n## 插入排序的优化： 希尔排序"},{"title":"7.2Sort(2)","url":"/2019/12/09/7.2Sort(2)/","content":"# 归并排序 & 快速排序  O（nlogn）\n\n## 归并排序（merge sort）\n> 归并排序采用分治思想，将整个数组从中间分成前后两部分，分别排好序后再合并到一起排序。\n\n![Alt](../img/mergeSort.jpg)\n\n> 分治是一种解决问题的处理思想，递归是一种编程技巧\n\n1. 是稳定的排序\n> merge函数中，值相同的元素在合并后没有改变先后顺序\n2. 时间复杂度\n> 把数组两两分开的时复是log\bN，合并过程中要遍历数组时复是N，两者属于嵌套关系，故归并排序的时复为O（NlogN）\n>\n> 归并排序的执行效率与原始数组的有序度无关，最好、最坏、平均时复都相同。\n3. 不是原地排序\n> 在合并函数中，需要借助一个额外空间来存临时数组，每次使用完临时数组后就释放掉占用的空间，且每次只会有一个合并函数在执行，所以空间复杂度是O（n）\n\n\n## 快速排序\n> 快排思想：选取一个元素作为pivot（分区点， 一般选择最后一个元素），将小于pivot的元素放左边，大于pivot的元素放右边，使用递归方式进行分区，直至区间缩小为1\n\n![Alt](../img/quickSort.jpg)\n\n1. 不是稳定的排序\n> 在分区函数中存在元素交换位置，因此会改变相同元素的先后顺序\n2. 是原地排序\n3. 时间复杂度\n> 分区函数的时复是O（n）\n> 最好情况：每次pivot都是中位数，数组分割次数LogN，故时复O(nlogn)\n> 最坏情况：已排好序的数组，需要分割n次，O(n^2)\n> 大部分情况下快排的时复都是O(nlogn)，极端情况才会退化到O(n^2)\n\n\n## 归并与快排的区别\n![Alt](../img/merge_and_quick.jpg)\n\n> 归并排序的处理是自下而上的，先处理子问题，合并排序后，再处理父问题。\n>\n> 快速排序的处理是自上而下的，先分区排序，再处理子问题。\n>\n> 归并排序是稳定排序，但空复为O（n），这是致命缺点。 所以相比较，快速排序更受欢迎。\n快排在大部分情况下时复为O(nlogn)，极端情况才会退化到O(n^2)，且极端情况发生的概率很小，并且我们可以选择合适的pivot来避免极端情况的发生。\n\n\n## 如何在O（n）时间复杂度内查找无序数组的第K大元素（从小到大排序后的第k个元素）？\n\n采用快排分区的思想，选取pivot分区，分区点为p，\n若k = p+1, pivot就是第k大元素\n k > p+1, 继续在[p+1, r]中查找\n k < p+1, 继续在[l, p-1]中查找\n\n 这样，遍历的次数分别是n、n/2、n/4、n/8 ... 直到区间缩小为1\n 所以总的遍历次数为（n + n/2 + n/4 + n/8 + ... + 1）= 2n - 1, 故时复为O（n）\n\n ## 现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？\n\n 利用上1GB的内存，且每个文件中的日志是排好序的，可以采用归并排序的方法，\n 每次分别从10个文件中取一批数据进行归并排序，将排序结果写入新的日志文件中，释放空闲内存，再重复读取与归并排序操作。"},{"title":"7.3Sort(3)","url":"/2019/12/09/7.3Sort(3)/","content":"# 线性排序：如何根据年龄给100万用户数据排序？\n\n时间复杂度是线性的（O（n）），即为线性排序。\n今天学习三种线性排序：桶排序、计数排序、基数排序。之所以能做到时间复杂度是线性的，是因为这三种排序都不是基于比较的排序算法，不涉及元素之间的比较操作。\n\n重点掌握这三种排序的适用场景\n\n## 桶排序（Bucket Sort）\n\n核心思想：将要排序的数据分别装入几个有序的桶中，桶内对数据进行排序，排好序后按照桶的顺序依次取出数据，组成的序列就是有序的了。\n\n![Alt](../img/bucketSort.jpg)\n\n### 时间复杂度为什么是O（n）？\n\n假设共有n个数据要排序，被均匀的装入m个有序桶中，每个桶就有k = n / m个数据，每个桶内部使用快速排序，时复为O（k * log k）， m个桶的时复就是O（m * k * log k）， 即O（n * log n/m）。当桶的个数m接近于数据个数n时，  log n/m 就是一个很小的常量， 时复也就接近O（n）。\n\n### 桶排序对排序数据有什么要求？\n\n1. 要排序的数据需要很容易地划分为m个桶，且桶与桶之间有着天然地大小顺序。这样的话，待桶内排序后，整个数据集就有序了。\n2. 数据在各个桶中的分布要比较均匀。如果有的桶数据很小，有的桶数据非常多，时复就不是常量级的。当所有数据都划分到一个桶中，时复就退化为O（nlogn）\n3. *桶排序比较适用于外部排序中。* 所谓的外部排序就是，数据存储在外部磁盘中，数据量较大，内存有限，不能将全部数据加载进内存中。\n\n### 假设有10GB的订单数据，按照订单金额进行排序，而内存只有几百MB，如何借助桶排序的思想来处理这个问题？\n\n1. 扫描一次数据，找出数据范围，便于划分桶。 假设扫描后发现最小金额为1元，最大金额为10万元。我们划分为100个桶，每个桶中金额范围跨度是1000。如第一个桶存放1-1000订单金额的数据，第二个桶存放1001-2000订单金额的数据... 每个桶对应一个文件，给文件依次命名为00、01、...、99。\n\n2. 理想情况下，所有数据均匀划分到100个文件中，每个文件大概100MB。依次将每个文件放入内存中，使用快排进行排序。每个文件都排好序后，按照文件编号依次读取数据到内存，然后再写入一个新的文件中，那这个新文件中的数据就是排序结果。\n\n3. 实际情况中并不能均匀地划分，可能某个桶内的数据特别多，内存无法一次读取。假如1-1000内的订单数据非常多，我们可以再利用桶排序将这些数据划分到1-100、101-200、...901-1000的桶中进行排序。若某个范围内的数据依然很多，无法一次性读入内存，那就继续划分，直到所有文件都能读入内存为止。\n\n## 计数排序（Counting Sort）\n\n计数排序是桶排序的一种特殊情况。\n\n当n个待排序的数据所处的范围并不大时，最大值为k， 则可以划分为k个桶，每个桶中的数据值是相同的，就省去了桶内排序的时间。例如高考的查分系统，总分700分，可以划分为701个桶，对应0-700分。只涉及一次遍历操作，所以计数排序的时复为O（n）。\n\n\n### 为什么叫“计数”排序？\n\n假如有8个考生，分数为0-5分，这8个考生的得分依次为：2、5、3、0、2、3、0、3，则共有6个桶，对应着数组C的下标0-5， 故C[6] = {2, 0, 2, 3, 0, 1}, C[i]代表分数i对应的考生个数。\n\n### 如何快速计算每个分数对应的考生的存储位置？\n\n即要求考生排序后的有序数组为R[8]={0, 0, 2, 2, 3, 3, 3, 5}\n\n思路： 对C[6]数据求和， C[i]里存放分数小于等于i的考生个数\nC[6] = {2, 2, 4, 7, 7, 8}\nC[3] = 7 表示分数<=3 的考生有7个\n\n算法过程：从后往前遍历原始数据数组A，取到第一个元素为3， 从数组C找到C[3]=7, 第7个元素即对应下标为6， 即填入R[6]=3, C[3]-1; 取到第二个元素为0， 从数组C找到C[0]=2, 第2个元素即对应下标为1， 即填入R[1]=2, C[0]-1; 按此步骤依次将数据放入R数组中。\n\n![Alt](../img/countingSort.jpg)\n\n> 遍历数组从后往前取，保证了计数是稳定排序算法。 当然从前往后取，结果也是一样的，但不稳定。\n\n> 计数排序只适用于数据范围不大的场景， 如果数据范围k比要排序的数据个数n大很多，就不适用了。 并且计数排序的数据都要是非负整数。\n\n\n## 基数排序（Radix Sort）\n\n假如要对10万个手机号码进行从小到大排序，要做到时复为O（n）的排序。手机号是11位的，范围太大，不适用于桶排序和计数排序。\n\n特点： 先比较高位，若高位大就不用再比较了，若高位相等则依次比较下一位。\n\n借助稳定排序算法， 想到按时间和金额对订单数据进行排序， 先对第11位数字进行排序，再对第10位数字进行排序。经过11次排序后，就得到最终的排序结果，每次排序中可以使用桶排序，桶内部的数据相同不用再排，故时复为O（n），11次排序的时复依然为O（n）\n\n例如对牛津字典中的单词进行排序，单词又长又短，我们可以在短的单词后面补“0”，任何字母的ASCAII值都大于0， 然后通过基数排序。\n\n> 基数排序对数据有一定的要求，需要将数据分割出“位”来进行排序，而且位之间有递进关系，若高位数据大就不必进行低位的比较了。同时，每一位的数据范围不能太大，可以使用桶排序或计数排序，做到线性排序。\n\n## 如何根据年龄给100万用户数据排序？\n设定最小年龄1岁，最大年龄120岁，进行桶排序，划分数据到120个桶，然后依次遍历这120个桶，就将数据排好序了。\n\n"},{"title":"6.Recursion","url":"/2019/12/09/6.Recursion/","content":"# 递归\n> 去的过程叫“递”， 回来的过程叫“归”\n\n## 递归满足的三个条件 \n1. 一个问题的解可以分解为几个子问题的解\n2. 这个问题与分解后的子问题，除了数据规模不同，求解思路要一样\n3. 存在递归终止条件\n\n## 写出递推公式，找出终止条件\n> 例如上台阶问题，一次只能上1阶或者2阶，问上n阶有几种走法?\n * f(n) = f(n-1) + f(n-2)\n * f(1) = 1\n * f(2) = 2\n> 则函数如下：\n```\nint f(int n){\n    if(n == 1)  return 1;\n    if(n == 2)  return 2;\n    return f(n-1) + f(n-2);\n}\n```\n\n## 递归代码要警惕堆栈溢出\n> 函数调用使用栈来保存临时变量。每调用一个函数，都会将临时变量封装成一个栈帧压入内存栈，等函数执行完返回时才出栈。 系统栈和虚拟机栈空间一般都不大，如果递归求解的规模较大，递归层次较深，不断地压入栈，会有堆栈溢出的风险。\n\n> 如何避免堆栈溢出？\n* 限制递归调用的最大深度（计算递归次数，到达阈值则抛出异常。 ）\n> 但这种做法并不能完全解决问题，因为最大允许的递归深度跟当前线程剩余的栈空间大小有关\n\n## 递归代码要警惕重复计算\n> 上台阶的问题其实是会重复计算,可以引入散列表来存储计算过的值\n```\nint f(int n){\n    if(n == 1)  return 1;\n    if(n == 2)  return 2;\n\n    //hasSolvedList是一个Map， key为n， value为f(n)\n    if (hasSolvedList.containsKey(n)) { \n        return hasSolvedList.get(n); \n    }\n\n    int ret = f(n-1) + f(n-2);\n    hasSolvedList.put(n, ret);\n    return ret;\n}\n```\n\n\n## 将递归代码改写成非递归代码\n* 递归的利：代码表达力强，代码简洁\n* 递归的弊：重复计算、堆栈溢出、复杂度高、过多的函数调用会耗时较多\n\n用迭代改写递归代码，相当于“手动”递归\n\n## 递归代码调试\n* 打印日志发现递归值\n* 结合调试断点\n"},{"title":"5.Queue","url":"/2019/12/09/5.Queue/","content":"# 队列\n#### 先进先出， 队尾入队，对头出队， 和栈一样是操作受限的线性表结构\n#### 顺序队列：用数组实现（tail == size 时， 整体数据一次搬移)\n#### 链式队列：用链表实现\n\n## 环形队列\n> 队空： head == tail\n>\n> 队满： head == （tail + 1）% size\n>\n> 队满时，tail指向最后一个空位置，环形队列会浪费一个内存空间\n\n## 阻塞队列和并发队列\n> 阻塞队列就是在队列的基础上增加了阻塞操作。简单来说，当队列为空时，出队操作就会被阻塞，\n直到有队列中有数据才能返回。当队满时，入队操作会被阻塞，直到队列中有空闲位置再插入数据再返回。\n>\n> 基于阻塞队列可以实现“生产者-消费者”模型。\n>\n> 并发队列：指线程安全的队列。\n\n## 线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？\n> 1. 采用非阻塞方式，直接拒绝请求\n> 2. 采用阻塞方式，使用阻塞队列将请求排队，等到有空闲线程时，取出排队中的请求进行处理\n    > * 若用链表实现阻塞队列，则是实现一个支持无限排队的无界队列，可能导致过多的请求排队，\n    处理请求的响应时间过长。所以针对响应时间比较敏感的系统，链式阻塞队列不太合适。\n    > * 若用数组实现阻塞队列，则是实现一个支持有限排队的有界队列。当线程池中排队的请求超过队列大小时，\n    则拒绝接下来的请求。所以顺序阻塞队列更适合响应时间敏感的系统。但要设置合理的队列大小， 太大则导致\n    等待请求过多，太小会使得资源利用不充分，没有发挥最大性能。\n\n#### 实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。"},{"title":"4.Stack","url":"/2019/12/09/4.Stack/","content":"# 栈\n## leetcode：20,155,232,844,224,682,496.\n#### 后进先出， 是一种操作受限的线性表结构\n\n## 如何实现一个栈？\n> 1. 顺序栈：用数组实现\n> 2. 链式栈：用链表实现（从头部push，pop，时复均为O（1））\n\n## 函数调用栈\n> OS 为每个线程分配一块独立的内存空间，这个内存就是以栈的结构来存储函数调用中的临时变量\n\n## 栈在表达式求值中的应用\n> 对于四则运算， 可以采用两个栈， 栈A存放操作数，栈B存放运算符。按表达式从做到右遍历入栈，当取到运算符时， 判断当前运算符与栈顶运算符的优先级， 如高则入栈，若低则取出栈顶运算符与两个操作数，计算后将结果入栈。\n\n## 检查括号是否匹配正确\n\n## 实现浏览器的前进、后退功能\n> 用两个栈来实现， 若依次访问网页a,b,c, 则栈1入栈abc， 此时后退，则栈1出栈c,  栈2入栈c。目前网页是b，若再访问d， 则栈1 为abd， 栈2需清空栈"},{"title":"3.LinkedList","url":"/2019/12/09/3.LinkedList/","content":"# 如何用链表来实现 LRU 缓存淘汰策略呢？\n> 常见 CPU缓存、数据库缓存。 缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？这就需要缓存淘汰策略来决定。常见的策略有三种：先进先出策略 FIFO（First In，First Out）、最少使用策略 LFU（Least Frequently Used）、最近最少使用策略 LRU（Least Recently Used）。\n\n#### 数组：连续的内存空间，对内存要求高。\n#### 链表：零散的内存块\n\n## 单链表\n![Alt](../img/singleLinkedList.jpg)\n\n#### 单链表的结点只包含数据和后继指针\n#### 头结点：记录链表的基地址\n#### 尾结点：指针指向空地址NULL\n\n### 插入与删除\n> 数组：为了保证内存的连续性，数组插入与删除需要搬移数据，时复为O（n）\n>\n> 链表：本身就是零散的内存空间串在一起，链表插入与删除不用搬移数据，只是改变结点的后继指针，时复为O（1）\n\n### 查找\n> 数组：内存的连续性保证了根据下标随机访问数据的时复为O（1）\n>\n> 链表：只知道链表的基地址，要查找第k个数据就需要从头结点开始遍历，时复为O（n）\n\n## 循环链表\n![Alt](../img/circularLinkedList.jpg)\n\n#### 尾结点的next指针指向头结点，具有环形结构\n\n## 双向链表\n![Alt](../img/doubleLinkedList.jpg)\n\n#### 每个结点都有一个prev前驱指针和一个next后继指针\n#### 因为多了前驱指针，会占更多的内存。但支持双向遍历，链表操作更灵活\n\n### 删除\n1. 删除结点中“值等于给定值”的结点\n> 无论是单链表还是双向链表，此种删除操作都需要先遍历链表（时复O（n）），然后删除找到的结点（时复O（1））。根据时复的加法法则，删除操作的时复为O（n）。\n\n2. 删除给定指针指向的结点（已知要删除的结点）\n> 单链表：删除结点q需要之前它的前驱结点，但单链表不能直接获取前驱结点，需要遍历链表找到前驱结点才能进行删除操作，p -> next = q , 说明p是q的前驱结点。时复为O（n）。\n>\n> 双向链表： 支持直接获取前驱结点。时复为O（1）\n\n### 使用了空间换时间的思想，Java的LinkedHashMap使用了双向链表\n\n## 双向循环链表\n![Alt](../img/doubleCircularLinkedList.jpg)\n\n## 链表 VS 数组\n\n时复 | 数组 |  链表\n--|--|--\n插入/删除| O（n）| O（1）\n随机访问 | O（1）| O（n）\n\n> 数组和链表的选择，不能仅限于时间复杂度的对比，根据实际情况而定\n\n> 1. 数组简单易用，使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，访问效率更高。 链表使用的是零散的内存空间，对CPU缓存不友好。\n    > * CPU缓存是为了弥补内存访问速度过慢而CPU执行速度过快的差异而引入的。CPU在从内存读取数据时，会把读取到的数据加载到缓存中。而CPU从内存中读取的并不只是特定地址的数据，而是读取一个小数据块，并保存在缓存中。在CPU下次访问内存数据时，会先从缓存中查找，若没找到再去查找内存，若找到了就不用去访问内存了，这样就实现了比内存访问更快的缓存机制。\n    > * 数组利用的是连续缓存，所以CPU在从内存中读取特定地址的数据时，可以顺便读取之后连续的一小块内存加载到缓存中，这样会比加载存储在链表中的数据快些。 所以说链表对CPU缓存不友好。\n> 2. 数组的缺点是大小固定，需要提前申请一定大小的内存空间。若申请size过大，浪费内存或导致“内存不足”（error）；若size过小，当空间已满后需要动态扩容，涉及内存申请和数据搬移，耗时。 而链表本身没有大小限制，天然支持动态扩容。\n> 3. 对内存十分敏感的更适合使用数组。链表需要存储指针，内存消耗会翻倍。且对链表频繁的插入与删除，会导致频繁地申请与释放内存，产生内存碎片。\n\n## 使用链表设计LRU缓存淘汰算法\n\n维护一个单链表，越接近链表尾部，是越早之前访问的数据。\n\n插入数据：\n1. 若数据存在于链表中， 则遍历链表找到数据所在结点，删除此结点，并将此结点添加到链表头部；\n2. 若数据不存在链表中\n    - 缓存未满，则直接生成该数据结点，并添加到链表头部。\n    - 缓存已满，删除尾结点，并将该数据结点添加到链表头部。\n\n基于链表的缓存访问都需要遍历数据，故时复为O（n）\n\n优化方案：引入散列表（Hash Table）来记录数据的存放位置，将缓存访问将为O（1）\n\n## 如何轻松写出正确的链表代码？\n1. 理解指针或引用的含义\n> C 或 C++中含有指针，Python 和 Java 有引用。 指针和引用的含义都是存储所指对象的内存地址。\n>\n> 对于指针的理解： 将变量赋值给指针，实际上是把变量的内存地址存储在指针中。指针指向这个变量，通过存储的内存地址便找到了这个变量。\n\n2. 警惕指针丢失和内存泄漏\n> 在进行链表操作时，要注意当前指针指向哪个结点，该结点的前驱或者后继结点是什么。 对于C语言来说，删除结点需要释放内存，否则会造成内存泄漏。\n```\n    // 在 a、b结点之间插入x结点，当前指针p指向a结点\n    p -> next = x;  // a结点指向x\n    x -> next = p -> next; // x 应该指向b， 但此时p -> next 不等于b\n```\n> 以上代码造成指针丢失， 正确写法是交换两行代码。\n\n3. 利用哨兵简化实现难度\n\n```\n// 在结点p后插入新结点\n// 头结点\nif(head == null)\n    head -> next = new_node\n\nnew_node -> next = p -> next\np -> next = new_node\n```\n\n```\n// 删除p结点的后继结点\n// 链表中只有一个结点\nif(head -> next == null)\n    head == null\n\np -> next = p -> next -> next\n```\n> 对于链表的插入和删除操作，需要对头尾结点做特殊处理，使得代码繁琐。对此，我们可以引入哨兵进行代码简化。将哨兵放在头部，head指针指向哨兵结点，这叫带头链表。反之，不带哨兵结点的叫做不带头链表。\n\n4. 重点留意边界条件的处理\n> 如果链表为空\n> 链表只包含一个结点\n> 链表只包含两个结点\n> 对于头尾结点的逻辑处理，是否正确\n\n5. 举例画图，辅助思考\n6. 多写多练，没有捷径\n\n\n\n\n## 思考\n1. 若字符串存储在链表中，如何判断是否是回文字符串？\n> 通过快慢指针找到链表的中间结点，然后反转后半段链表，再与前半段链表依次进行比较\n\n2. 常见链表操作：\n    * 单链表反转(迭代&递归)\n    * 链表中环的检测 （哈希表）\n    * 两个有序的链表合并 \n    * 删除链表倒数第 n 个结点（一次遍历使用快慢指针）\n    * 求链表的中间结点（快慢指针）\n"},{"title":"10.3HashTable(3)","url":"/2019/12/09/10.3HashTable(3)/","content":"# 为什么散列表和链表经常会一起使用？\n\n## LRU缓存淘汰算法\n\n利用链表实现：\n\n维护一个按照访问时间从小到大（按入队出队来说是从大到小）排序的链表，结点越靠近尾部，越是最近访问的数据。\n访问一个数据，若不存在于链表中，因为缓存大小有限，当缓存空间足够时，直接将新数据放在尾部；当缓存空间不足时，删除链表头部结点，并将新数据放在尾部。\n若存在于链表中，将该结点移动到尾部。\n因为查找数据需要遍历链表，所以基于链表实现的LRU缓存淘汰算法的时复为O（n）。\n\n一个缓存系统主要包括3个操作：\n1. 在缓存中查找一个数据\n2. 向缓存中添加一个数据\n3. 从缓存中删除一个数据\n\n采用散列表+链表的组合方式，可以将这3个操作的时复降为O（1）。\n\n![Alt](../img/LRU_hashtable_linkedlist.jpg)\n\n图中的散列表是通过链表法来解决散列冲突的，所以横向链表为散列表的拉链，同时结合着一条双向链表。故图中结点data指结点数据，prev和next指双向链表的前后指针， hnext指散列表拉链的下一指针。双向链表需要维护一个头结点、尾结点和结点个数（用来判满）。\n\n*查找* 通过散列函数计算散列值，找到对应的散列槽位，然后遍历拉链查找数据，拉链的结点个数k = n/m(散列表数据总个数/槽位个数)， 所以散列表查找操作的时复接近O（1）。找到数据后将它移动到双向链表的尾部，时复为O(1)。故缓存查找的时复接近O（1）。\n\n*添加* 先查找是否存在于缓存中。若存在，将该数据移动到链表尾部。若不存在，当缓存已满时，删除双向链表的头结点，并将该数据加到链表尾部；当缓存未满时，直接将该数据加到链表尾部。时复接近O（1）\n\n*删除* 先在缓存中找到该结点， 然后从双向链表中删除该结点，双向链表中存放了前后指针，所以找到该结点后直接删除结点的时复为O（1）。\n\n\n## Java LinkedHashMap\n\nJava的HashMap是通过散列表实现的 ，但散列表存储是乱序的， LinkedHashMap继承了HashMap，linked代表的是双向链表，做到了按顺序存储，且最新访问的数据会放到链尾。\n\n```\n\n// 10是初始大小，0.75是装载因子，true是表示按照访问时间排序\nHashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);\nm.put(3, 11);\nm.put(1, 12);\nm.put(5, 23);\nm.put(2, 22);\n\nm.put(3, 26);\nm.get(5);\n\nfor (Map.Entry e : m.entrySet()) {\n  System.out.println(e.getKey());\n}\n\n```\n\n打印结果是1,2,3,5\n\n首先四个put后，双向链表的存放顺序是3,1,5,2。\n\n接着m.put(3, 26)，查询找到key=3的结点，将该结点移动到链表尾部（包含value值修改），存放顺序变成1,5,2,3。\n\n接着m.get(5)，查询找到key=5的结点，将该结点移动到链表尾部，故存放顺序为1,2,3,5.\n\n按照访问时间排序的LinkedHashMap实际上就是一个支持LRU缓存淘汰算法的缓存系统。\n\n\n## 思考\n1. 今天讲的几个散列表和链表结合使用的例子里，我们用的都是双向链表。如果把双向链表改成单链表，还能否正常工作呢？为什么呢？\n\n使用双向链表是为了维护前后指针，做到不用遍历链表，便可直接插入尾结点和删除结点，时复为O（1）。 而使用单链表插入尾结点或者删除结点的时复为O（n），如果硬要做到O（1），在查找结点时使用双指针记录要删除结点的前一结点。\n\n2. 假设猎聘网有 10 万名猎头，每个猎头都可以通过做任务（比如发布职位）来积累积分，然后通过积分来下载简历。假设你是猎聘网的一名工程师，如何在内存中存储这 10 万个猎头 ID 和积分信息，让它能够支持这样几个操作：\n    - 根据猎头的 ID 快速查找、删除、更新这个猎头的积分信息；\n    - 查找积分在某个区间的猎头 ID 列表；\n    - 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表。\n\n根据ID和猎头信息对象建立散列表，快速查找、删除、更新的时复是O（1）。\n根据积分和猎头信息对象数组建立跳表（积分从小到大排序的链表+索引），查找的时复是O（logn）。\n只能遍历跳表中维护的链表，时复为O（n），效率极低。\n"},{"title":"10.2HashTable(2)","url":"/2019/12/09/10.2HashTable(2)/","content":"# 如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？\n\n> 散列表的查询效率并不能笼统地说是O（1）， 它跟散列函数、装载因子、散列冲突等都有关。 如果散列函数设计得不好，或装载因子过高，都会使得发生散列冲突的概率升高，查询效率下降。\n>\n> 极端情况下，恶意攻击者精心设计散列函数，使得所有查询数据经过散列函数后都散列到同一个槽里。如果我们使用的是基于链表的解决散列冲突的方法，那么散列表则退化成链表，查询效率也由O（1）急剧退化为O（n）\n>\n> *散列表碰撞攻击的基本原理：*\n> 假如散列表中有10万条数据，基于链表解决散列冲突的极端情况下，散列表退化为链表， 查询效率下降10万倍。也就是说如果之前100次查询只需要0.1秒，那么现在就需要1万秒。这样会因为查询操作消耗大量CPU和线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（Denial of Service， Dos）的目的。\n\n## 如何设计散列函数\n\n1. 散列函数设计得不能太复杂\n> 散列值计算过于复杂，会影响查询操作的效率，影响散列表的性能。\n2. 计算得到的散列值尽可能随机且均匀分布\n> 避免或者最小化散列冲突。即使发生了散列冲突，散列到每个槽的数据也比较均匀，不会出现某个槽数据过多的情况。\n\n## 装载因子过大怎么办？\n\n> 对于静态数据，已知数据大小和规律，可以设计一个较好的散列函数来避免散列冲突。\n>\n> 对于动态数据，不知道数据的大小，所以我们提前申请的是一个一定大小的散列表空间，当插入的数据越来越多，空闲位置越少，装载因子就会越大，发生散列冲突的概率上升。 \n>\n> 当装载因子过大时，可以进行散列表动态扩容。 假设扩容为原空间的两倍，则原来0.8的装载因子就会降为0.4。\n>\n> 针对散列表扩容，搬移数据要复杂很多。因为空间大小变了，内存地址也变了，需要根据散列函数重新计算出每个数据的存储位置\n> \n> 对于支持动态扩容的散列表，插入一条数据时，最好情况不需要扩容直接插入，最好时复为O（1）。最坏情况先扩容再插入数据，需要申请内存，重新计算哈希位置，搬移数据，最坏时复为O（n）。 用摊还分析法，平均时复为O（1）。\n> \n> 对于删除操作，随着删除的数据越来越多，空闲位置越多，对于内存消耗有要求的，可以考虑散列表缩容\n\n## 如何避免低效地扩容\n\n> 最坏情况下插入一条数据前需要动态扩容，而申请内存，计算哈希位置和搬移数据的操作是很耗时的，插入操作太慢，会影响用户体验。这时，“一次性”的扩容机制就不合适。我们可以考虑把这几个步骤分开分批处理。\n>\n> 当装载因子达到阈值后，申请新空间，并不要立即搬移数据。当有新数据插入时，我们将新数据插入到新散列表中，并将旧散列表的一条数据插入到新散列表中。 这样就把扩容操作穿插在插入操作中，提升效率。\n>\n> 这种方式扩容期间，若查询数据，需将兼容新旧散列表的数据。先查找新散列表，没找到再查找旧散列表。\n>\n> 通过均摊方式，将数据搬移操作穿插在插入操作，把一次性扩容的代价均摊到多次插入操作之中，这样任何时刻插入一个数据的时复都是O（1），提升了散列表性能。\n\n## 如何选择冲突解决方法\n\n> 开放寻址法和链表法很常用，Java的LinkedHashMap采用了链表法解决冲突，ThreadLocalMap采用了通过线性探测的开放寻址法解决冲突\n\n### 开放寻址法\n> 优点: \n> 1. 数据存储在数组中，可以利用CPU缓存加快查询速度。\n> 2. 数组实现的散列表，序列化简单，而链表法包含指针，序列化比较复杂\n>\n> 缺点：\n> 1. 删除数据比较麻烦，需要特殊标记已经删除的数据。\n> 2. 存放数据变多时，发生散列冲突的概率较大。所以使用开放寻址法解决冲突的散列表，装载因子的上限不能过大，\n> 3. 这样又比链表法更浪费内存空间。\n>\n> 总结： 数据量小，装载因子小，适用开放寻址法。 这就是Java的ThreadLocalMap采用了线性探测的开放寻址法解决冲突的原因。\n\n### 链表法\n> 优点: \n> 1. 对内存空间的利用率较高。 这也是链表优于数组的地方。\n> 2. 对大装载因子的容忍度高。开放寻址法只适用装载因子小于1的情况，当接近1时，可能会有大量的散列冲突，导致大量的探测、再散列等等，性能下降。对于链表法，只要散列函数计算出的值随机均匀分布，即使装载因子大于1， 也只是增加了链表的长度。查询效率虽然下降了，但比顺序查找快得多。\n>\n> 缺点:\n> 1. 因为链表要存放指针，所以对于比较小的对象的存储，比较消耗内存。但是当存放大的对象时，指针的内存消耗在大对象面前就可以忽略不计。\n> 2. 链表的结点不是连续分布，对CPU缓存不友好，对执行效率也有一定的影响。\n> \n> 总结：\n> 1. 对链表法进行改造。将链表法中的链表换成其他高效的动态数据结构（跳表、红黑树等）,查找时间降为O（log n）。这样也就有效地避免了散列碰撞攻击。\n> 2. 基于链表的散列冲突处理方法适用于存储大对象、大数据量的散列表。而且比起开放寻址法，更加灵活，支持更多优化策略，比如用黑红树代替链表。\n\n\n## 何为一个工业级的散列表？工业级的散列表应该具有哪些特性？\n1. 支持快速的查询、插入、删除操作\n2. 内存占用合理，不能浪费太多的内存空间\n3. 性能稳定，极端情况下，散列表的性能也不能退化到无法接受的程度\n\n## 如何实现工业级单列表\n1. 设计一个合适的散列函数\n2. 定义装载因子阈值，并设计动态扩容策略\n3. 选择合适的散列冲突处理方式\n"},{"title":"1.复杂度分析","url":"/2019/11/30/1.ComplexityAnalysis/","content":"\n计算机讲求的是运行的更快，更省内存空间，也就是对执行效率（时间）和资源消耗（空间）的要求。故在写算法时，要进行时间复杂度和空间复杂度分析，才能评断算法的好坏。\n\n<!-- more -->\n\n## 为什么要复杂度分析\n\n直接拿算法进行测试，得到执行时间和占用空间大小，这叫*事后统计法*\n1. 测试结果受测试环境影响\n2. 测试结果受数据规模影响\n \n我们需要用一个不用具体数据进行测试，就能估计出算法的执行时间和占用空间的方法， 即进行时间、空间复杂度分析。\n\n## 复杂度分析的方法（大O复杂度表示法）\n### 时间复杂度分析\n> 时间复杂度（Time complexity）：代码执行时间随数据规模增长的变化趋势\n\n1. 关注循环执行次数最多的一段代码\n2. 加法法则： 总时间复杂度 = 量级最大的那段代码的复杂度\n    > 给定已知的循环次数，即使是10000次，十万次，也是常量级时复\n\n    ```\n    int cal(int n) { \n        int sum_1 = 0; \n        int p = 1; \n        for (; p < 1000; ++p) { \n            sum_1 = sum_1 + p; \n        } \n        \n        int sum_2 = 0; \n        int q = 1; \n        for (; q < n; ++q) { \n            sum_2 = sum_2 + q; \n        } \n        \n        int sum_3 = 0; \n        int i = 1; \n        int j = 1; \n        for (; i <= n; ++i) { \n            j = 1; \n            for (; j <= n; ++j) { \n                sum_3 = sum_3 + i * j; \n            } \n        } \n        \n        return sum_1 + sum_2 + sum_3; }\n    ```\n\n    > 第一段代码循环大概1000次，也是常量级，时复O（1）; 第二段循环n次，时复O（n）； 第三段循环n^2次，时复O（n^2）。 所以整个函数的时复为O（n^2）\n\n3. 乘法法则：嵌套代码复杂度 = 嵌套代码内外循环复杂度的乘积\n\n### 常见时间复杂度实例分析\n![Alt](/img/algo/ordinaryComplexity.jpg)\n1. 将对数阶O（logn）循环n次， 就是O（nlogn）\n2. O（m + n）， O（m * n） 数据规模m、n均未知\n\n### 空间复杂度分析\n空间复杂度（Space complexity）：算法的存储空间随数据规模增长的变化趋势\n常见空间复杂度： O（1） O（n） O（n^2）\n\n## 最好最坏时间复杂度\n## 平均时间复杂度\n\nfind（）：假设查找x在数组中的位置，存在0 ~ n-1位置上和不在数组中，共n+1中情况，那么平均情况的复杂度=（1+2+...+n+n）/（n+1）= n（n+3）/ 2(n+1)， 去除系数、低阶、常量，时复为O（n）\n\n以上计算方法是错误的，没有考虑每种情况的概率。x是否存在于数组的概率各占1/2， x存在于0 ~ n-1位置上的概率各为1/2n, 所以（1+2+...+n）* 1/2n + n * 1/2 = 3n + 1 / 4, 故平均时复为O（n）。加入概率的叫做加权平均值（期望值），故称为加权平均时间复杂度,简称平均时间复杂度。\n\n## 均摊时间复杂度\n\n```\n // array表示一个长度为n的数组\n // 代码中的array.length就等于n\n int[] array = new int[n];\n int count = 0;\n \n void insert(int val) {\n    if (count == array.length) {\n       int sum = 0;\n       for (int i = 0; i < array.length; ++i) {\n          sum = sum + array[i];\n       }\n       array[0] = sum;\n       count = 1;\n    }\n\n    array[count] = val;\n    ++count;\n }\n```\n> 这段代码的功能是：数组没满时，直接插入元素；数组已满时，计算所有元素的和，清空数组，并将和存放在array[0],继续插入元素。\n> 1. 最好时复：数组没满，直接插入，O（1）\n> 2. 最坏时复：数组已满，先求和再插入语， O（n）\n> 3. 平均时复：根据插入的位置不同，分为0 ~ n-1， 和数组已满插入元素，共n+1种情况，每种情况发生的概率是1 / n+1。 而前n种的时复是O（1）， 最后一种的时复是O（n）， 所以1 * 1/n+1 + ... + 1 * 1/n+1 + n * 1/n+1 = 2n / n+1, 故平均时复为O（1）\n> 4. 什么时候使用平均时复或是均摊时复？\n>      \n>    find（）和insert（）的对比：find（）最好情况为O（1），insert（）在大部分情况下为O（1），极端情况才为O（n）。 inset（）的时复出现是有规律的， 一次O（n）插入操作后会有n-1次的O（1）插入操作，针对这种特殊场景，不再使用较复杂的平均时复分析法， 而是采用摊还分析，求均摊时间复杂度。\n> 5. 均摊时复：将耗时最多的一次O（n）操作，均摊到n-1次耗时少的O（1）操作，那么每次操作的时复都是O（1）。故均摊时复为O（1）。\n \n### 什么时候用摊还分析法来估计算法的均摊时间复杂度？\n\n当在特殊场景下，算法操作的时复有规律地分布着，看看是否能够把较高时间复杂度的操作均摊到较低时复的操作上。一般情况，均摊时复等于最好时复。 均摊时复是一种特殊的平均时复。\n\n## 思考题\n\n```\n\n// 全局变量，大小为10的数组array，长度len，下标i。\nint array[] = new int[10]; \nint len = 10;\nint i = 0;\n\n// 往数组中添加一个元素\nvoid add(int element) {\n   if (i >= len) { // 数组空间不够了\n     // 重新申请一个2倍大小的数组空间\n     int new_array[] = new int[len*2];\n     // 把原来array数组中的数据依次copy到new_array\n     for (int j = 0; j < len; ++j) {\n       new_array[j] = array[j];\n     }\n     // new_array复制给array，array现在大小就是2倍len了\n     array = new_array;\n     len = 2 * len;\n   }\n   // 将element放到下标为i的位置，下标i加一\n   array[i] = element;\n   ++i;\n}\n```\n> 最好时复：数组未满，直接添加，O（1）\n> \n> 最坏时复：数组已满，扩容搬移数据后再添加，O（n）\n> \n> 摊还时复： 将搬移数据的操作均摊到前n-1次直接添加的操作，时复等于直接添加的时复O（1）\n> ","tags":["数据结构与算法"],"categories":["数据结构与算法之美"]},{"title":"Hello World","url":"/2019/11/12/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n<!-- more -->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"}]